\documentclass[../main.tex]{subfiles}

\begin{document}

\textbf{Scale Mixture Of Normals (SMiN) Representation}

In foreign exchange markets such as the EUR/USD, market moves are widely regarded to be symmetric in nature \cite{tankov2003financial}. This motivates the selection of the parameter $\beta = 0$. Instead of considering the general $\alpha$-stable process, we can instead restrict ourselves to a more analytically tractable sub-class, the a Symmetric $\alpha$-stable (S$\alpha$S) process instead.

For $\beta = 0$, we have a convenient Scale Mixture of Normals (SMiN) representation based off the product property of $\alpha$-stable distributions: If X and Y are independent random variables with $\lambda_1 \sim S_{\alpha/2}(1, 1, 0)$ and $\lambda_2 \sim S_2(1,0,0) = \mathcal{N}(0,1)$, then $\lambda_1 \lambda_2 \sim S_\alpha(1, 0, 0)$.

We can then convert the discretised state space model specified in \autoref{eq:2__2__1__drift_corrected_simple_PF} into SMiN form as specified in \autoref{eq:2__2__2__RBPF_model} below.

\begin{equation}
    \underbrace{
        \begin{bmatrix}
        x_{1,t+\delta t} \\ x_{2,t + \delta_t}
        \end{bmatrix}
    }_{\mathbf{S_{t + \delta t}}}
    =
    \underbrace{
        \begin{bmatrix}
        1 & \delta t \\ 0 & e^{\theta \delta t}
        \end{bmatrix}
    }_{\mathbf{A}}
    \underbrace{
        \begin{bmatrix}
        x_{1,t} \\ x_{2,t}
        \end{bmatrix}
    }_{\mathbf{S_t}}
    + 
    \underbrace{
        \begin{bmatrix}
        0 \\ \sigma_t \sqrt{\lambda_{1, t-1}}
        \end{bmatrix}
    }_{\mathbf{b}} \lambda_{2, t};
    \qquad \lambda_{2,t} \sim \mathcal{N}(0,1) \text{,   } \lambda_{1,t} \sim S_{\alpha/2}(1,1,0)
    \label{eq:2__2__2__RBPF_model}
\end{equation}

This means that conditional upon us observing $\lambda_1$, $\lambda_2$ is gaussian, making inference much more tractable. We exploit this fact by designing a Rao-Blackwellised Particle Filter to sample $\lambda_1$ by using a simple particle filter, and propogating the states using a Kalman Filter conditioned upon the sampled $\lambda_1$ of the particle. 

We take a short detour to properly formulate the inference problem using the SMiN representation here. In the inference problem, we are seeking to infer the state variables $\mathbf{s}_t$ given the observations $y_t$. We assume model dynamics as follows:

\begin{eqnarray}
    \mathbf{s}_{t} & = & \mathbf{A} \mathbf{s}_{t - \delta t} + \mathbf{b}\sqrt{\lambda_t} \eta_t \notag \\
    y_t  & = & \mathbf{C} \mathbf{s}_t + d \epsilon_t \\
    \lambda_t & \sim & S_{\alpha / 2}(1,1,0) \notag \\
    \eta_t, \epsilon_t & \sim & \mathcal{N}(0,1) \notag 
\end{eqnarray}

It will also be useful to use the following expression for $y_t$:

\begin{eqnarray}
    y_t & = & \mathbf{CA} \mathbf{s}_{t - \delta t} + \underbrace{
        \begin{bmatrix}
            \mathbf{Cb} \sqrt{\lambda_t} & d
        \end{bmatrix}
    }_{\mathbf{e}_t} \underbrace{
        \begin{bmatrix}
        \eta_t \\ \epsilon_t
        \end{bmatrix}
    }_{\mathbf{n_t}} \\
    \mathbf{n}_t & \sim & \mathcal{N} \left(
        % \begin{bmatrix}0 \\ 0\end{bmatrix}
        \mathbf{0}
        , \mathbf{I} \right) \notag \\
    \lambda_t & \sim & S_{\alpha / 2}(1,1,0) \notag
\end{eqnarray}

\ToFix{This transforms our state equation form into a (nearly?) $\alpha$-stable sub-gaussian form. (This is NOT in $\alpha$-stable sub-gaussian form, see:  \href{https://statistik.econ.kit.edu/download/doc_secure1/SubGaussianDAX30-KRHF-FV.pdf}{$<$ $\alpha$-stable sub-gaussian definition $>$} for reference) }

\textbf{Generic Rao-Blackwellised Particle Filter (RBPF)}

For Rao-Blackwellised Particle Filtering, we partition the state vector into gaussian and non-gaussian components. We can then use standard Kalman Filtering to obtain optimal estimates for the gaussian state components, after obtaining estimates for the non-gaussian state components.  

We partition the state from \autoref{eq:2__2__2__RBPF_model} as $\vec{s}_t = (\vec{x}_t, \lambda_t).





Again, detailed treatment of the Rao-Blackwellised Particle Filter is omitted, but a short summary of the recursive steps taken every time step are presented below. 







\textbf{Adaptive Sampling}

One way of attempting to improve the performance of the RBPF is to form a better importance sampling distribution. In this section, we seek to form a better importance sampling distribution for $\pi(\lambda_t | y_{1:t}, \lambda_{0:t-1})$.

For optimality, we want $\pi(\lambda_t | y_{1:t} \lambda_{0:t-1}) \approx p(\lambda_t | y_{1:t}, \lambda_{0:t-1})$.


It is hard to formulate an analytical form for this density directly. To get around this, we can instead form the analytical joint distribution for $p(y_t, \lambda_t | y_{1:t-1}, \lambda_{0:t-1})$. The complete derivation is given in the appendix, with the key steps highlighted below. 

We first augment the probability density given above with $x_{0:t-1}$. This allows us to use results from the previously-performed Kalman Filter step.

\begin{eqnarray}
    & & p(y_t. \lambda_t, x_{0:t-1} | y_{1:t-1}, \lambda_{0:t-1}) \notag \\
    & \propto & p(y_t | y_{1:t-1}, \lambda_{0:t}, x_{0:t-1}) p(x_{0:t-1} | y_{1:t-1}, \lambda_{0:t}) p(\lambda_t | \lambda_{0:t-1}) \notag \\
    & \approx & p(y_t | y_{1:t-1}, \lambda_{0:t}, x_{0:t-1}) p(x_{0:t-1} | y_{1:t-1}, \lambda_{0:t-1}) p(\lambda_t | \lambda_{0:t-1}) \notag \\
    & = & \Gaussian (y_t | \mathbf{CAx}_{t-1}, \mathbf{ee}^T) \Gaussian (\mathbf{x}_{0:t-1} | \mathbf{\mu}_k, \mathbf{\Sigma}_k) S_{\alpha/2}(1,1,0) \notag \\
    & = & \Gaussian \left(
        \begin{bmatrix} y_t \\ \vec{x}_{t-1}  \end{bmatrix} | 
        \begin{bmatrix} \vec{CAx}_{t-1}  \end{bmatrix},
        \begin{bmatrix} \vec{ee}^T + (\vec{CA})\vec{\Sigma}_k(\vec{CA})^T & \vec{CA\Sigma}_k \\ \vec{\Sigma}_k(\vec{CA})^T & \vec{\Sigma}_k \end{bmatrix} 
    \right)  S_{\alpha/2}(1,1,0) \notag
\end{eqnarray}

We can then obtain the required density $p(y_t, \lambda_t | y_{1:t-1}, \lambda_{0:t-1})$ by marginalising out $x_{0:t-1}.



\OptionTwo{
    It is hard to formulate an analytical form for this density directly. To get around this, we can instead form the analytical joint distribution for $p(y_t, \lambda_t | y_{1:t-1}, \lambda_{0:t-1})$:
}


\begin{equation}
    p(y_t, \lambda_t | y_{1:t-1}, \lambda_{0:t-1}) = \mathcal{N}(y_t | \mathbf{CA} \mu_k , (\mathbf{Cb})(\mathbf{Cb})^T\lambda_t + d^2) S_{\alpha/2} (\lambda_t | 1,1,0) \label{eq:2__2__2__analytical_dist_original}
\end{equation}

By fixing the value of $y_t$ in \autoref{eq:2__2__2__analytical_dist_original}, we can obtain the desired distribution $p(\lambda_t | y_{1:t}, \lambda_{0:t-1})$. In general, there is no closed form expression for this distribution. 

This formulation of the joint distribution also gives us an alternative interpretation for the sampling density $p(\lambda_t | y_{1:t}, \lambda_{0:t-1})$. 

We can reinterpret this as finding the posterior density of $\lambda_t$
We can interpret this density as the posterior density of the variance of a normal distribution ($\lambda_t$)



However, there are a variety of methods to draw samples from this distribution.

\begin{enumerate}
    \item \textbf{Rejection Sampling}
    
    We adapt the methods of Godsill and Kuruoglu, 1999 to draw samples from this distribution.
    
    The target distribution for the rejection sampling scheme is given by:
    
    $$ f(y_t, \lambda_t | y_{1:t-1}, \lambda_{0:t-1}) = \mathcal{N}(y_t | \mathbf{CA} \mu_k , (\mathbf{Cb})(\mathbf{Cb})^T\lambda_t + d^2) S_{\alpha/2} (\lambda_t | 1,1,0) $$
    
    
    \OptionOne{
        
        Using a proposal distribution $g(\lambda_t | y_{1:t}, \lambda_{0:t-1}) = S_{\alpha/2}(1,1,0)$, we can bound the probability density from above using: 
        
        \begin{eqnarray}
            M & \geq & \frac{f(y_t, \lambda_t | y_{1:t-1}, \lambda_{0:t-1})}{g(y_t, \lambda_t | y_{1:t-1}, \lambda_{0:t-1})} \notag \\
            & = & \frac{1}{\sqrt{2 \pi v_t^2}} \exp (-0.5)
        \end{eqnarray}
    
    }
    
    \OptionTwo{
    
        Using a proposal distribution $g(\lambda_t | y_{1:t}, \lambda_{0:t-1}) = S_{\alpha/2}(1,1,0)$, we can use the likelihood as a valid rejection function as it is bounded from the above:
        
        \begin{eqnarray}
            \mathcal{N}(...) & \geq & \frac{f(y_t, \lambda_t | y_{1:t-1}, \lambda_{0:t-1})}{g(y_t, \lambda_t | y_{1:t-1}, \lambda_{0:t-1})} \notag \\
            & = & \frac{1}{\sqrt{2 \pi v_t^2}} \exp (-0.5)
        \end{eqnarray}
    
    }
    
    This gives a suitable rejection sampler as:
    
    \begin{enumerate}
        \item Draw $\lambda_t \sim S_{\alpha_2}(1,1,0)$
        \item Draw $ u \sim U \left( 0, \frac{1}{\sqrt{2 \pi v_t^2}} \exp (-0.5) \right)$
        \item If $u > \mathcal{N}(....)$, reject $\lambda_t$ and go to Step (a)
    \end{enumerate}
    
\end{enumerate}




\begin{equation}
    p(y_t, \lambda_t | y_{1:t-1}, \lambda_{0:t-1}) = \mathcal{N}(y_t - \mathbf{CA}\mathbf{\mu}_k | 0, \lambda_t')S_\frac{\alpha}{2}(\lambda_t' | (\mathbf{Cb})(\mathbf{Cb})^T, 1, \mathbf{dd}^T + \mathbf{CA\Sigma}_k(\mathbf{CA})^T)
    \label{eq:2__2__2__analytical_dist}
\end{equation}

In general, we cannot sample from \autoref{eq:2__2__2__analytical_dist} exactly. 



\end{document}